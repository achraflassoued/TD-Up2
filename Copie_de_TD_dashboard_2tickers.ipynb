{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de TD_dashboard_2tickers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocvIoR-mFya7"
      },
      "source": [
        "[texte du lien](https://)# **Twitter TD dashboard**\n",
        "\n",
        "Dates: 2021\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3j0nwGjGZBo"
      },
      "source": [
        "## Initial Setup\n",
        "\n",
        "- **Run \"Setup\" below first.**\n",
        "\n",
        "    - This will load libraries and download some resources that we'll use throughout the tutorial.\n",
        "\n",
        "    - You will see a message reading \"Done with setup!\" when this process completes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKVEnPi34qj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "579d3c94-f369-4abb-aeb8-952e98279791"
      },
      "source": [
        "#@title Setup (click the \"run\" button to the left) {display-mode: \"form\"}\n",
        "\n",
        "## Setup ##\n",
        "\n",
        "# imports\n",
        "\n",
        "# built-in Python libraries\n",
        "# -------------------------\n",
        "\n",
        "! pip install peony-twitter[all]\n",
        "! pip install peony-twitter[all]\n",
        "! pip show tornado\n",
        "! pip install nest_asyncio \n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "! pip install yahoo_fin\n",
        "! pip install requests_html\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import requests_html\n",
        "import lxml.html as lh\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "import time\n",
        "import json\n",
        "import datetime\n",
        "from peony import EventStream, PeonyClient, event_handler, events\n",
        "import asyncio\n",
        "import random\n",
        "import sys\n",
        "import csv\n",
        "import requests\n",
        "\n",
        "print(\"Done with setup!\")\n",
        "print(\"If you'd like, you can click the (X) button to the left to clear this output.\")\n",
        "\n",
        "# generate graph from twitter json\n",
        "def create_graph(data):\n",
        "    all_data = dict(data)\n",
        "    edge_list = []\n",
        "    user_screen_name = all_data['user']['screen_name']\n",
        "    timestamp_ms = all_data['timestamp_ms']\n",
        "    if len(all_data['entities']['hashtags']) > 0:\n",
        "        for index, i in enumerate(all_data['entities']['hashtags']):\n",
        "            edge_list.append((\"@\"+user_screen_name, \"#\"+all_data['entities']['hashtags'][index]['text'], timestamp_ms))\n",
        "    if len(all_data['entities']['user_mentions']) > 0:\n",
        "        for index, i in enumerate(all_data['entities']['user_mentions']):\n",
        "            edge_list.append((\"@\"+user_screen_name, \"@\"+all_data['entities']['user_mentions'][index]['screen_name'], timestamp_ms))\n",
        "    return edge_list\n",
        "  \n",
        "# compute components from reservoir of edges\n",
        "def components(edges):\n",
        "  global d\n",
        "  d={}\n",
        "  n=0\n",
        "  for e in edges:\n",
        "     a =e [0]\n",
        "     b = e[1]\n",
        "     if a in d.keys():\n",
        "        d[a].add(b)\n",
        "     else:\n",
        "      d[a]=set([b])\n",
        "     if b in d.keys():\n",
        "      d[b].add(a)\n",
        "     else:\n",
        "      d[b]=set([a])\n",
        "\n",
        "  #print (\"Dict=\", d, len(d))\n",
        "  n=len(d)\n",
        "  m=0\n",
        "  #Breadth-first search from first point, first component\n",
        "  a=list(d.keys())[0]\n",
        "\n",
        "  #dc keeps b:i  if i is the length of the shortest path from a to b in the first component\n",
        "  dc={}\n",
        "  dc[a]=0\n",
        "  for b in d[a]: \n",
        "    if b in dc.keys(): pass\n",
        "    else: dc[b]=1\n",
        "\n",
        "  #Initialize S ans S1, Start iterating\n",
        "  S=d[a]\n",
        "  comp=[]\n",
        "  S1=set([a])\n",
        "  S=S.union(S1)\n",
        "  #print(\"S=\",S)\n",
        "  while S > S1:\n",
        "      S1=S\n",
        "      for u in S:\n",
        "         S=S.union(d[u])\n",
        "         for v in d[u]:\n",
        "           if v in dc.keys():\n",
        "              dc[v]=min(dc[v],dc[u]+1)\n",
        "           else:\n",
        "            dc[v]=dc[u]+1\n",
        "  for u in S:\n",
        "       m=m+len(d[u])\n",
        "\n",
        "  comp.append((len(S),int(m/2),list(S)))\n",
        "    \n",
        "  #print(\"Component\",comp)\n",
        "\n",
        "  ST=S\n",
        "    \n",
        "  #print(\"ST=\",ST)\n",
        "\n",
        "  #The other components: origin must be outside ST, same treatment\n",
        "  i=1\n",
        "  while i<len(d):\n",
        "   m=0\n",
        "   while  list(d.keys())[i] not in ST:\n",
        "    a=list(d.keys())[i]\n",
        "    S=d[a]\n",
        "    S1=set([a])\n",
        "    S=S.union(S1)\n",
        "    while S > S1:\n",
        "      S1=S\n",
        "      for u in S:\n",
        "         S=S.union(d[u])\n",
        "    for u in S:\n",
        "       m=m+len(d[u])\n",
        "    comp.append((len(S),int(m/2),list(S)))\n",
        "    ST=ST.union(S)  \n",
        "   i+=1     \n",
        "  return comp\n",
        "\n",
        "\n",
        "def comp_edges(comp1):\n",
        " compedges=[]\n",
        " j0=0\n",
        " while j0<len(comp1):\n",
        "   l1 = comp1[j0][2]\n",
        "   j=0\n",
        "   cp=[]\n",
        "   while j <len(l1):\n",
        "     a=l1[j]\n",
        "     j1=0\n",
        "     while j1< len(d[l1[j]]):\n",
        "      f=(a,list(d[l1[j]])[j1])\n",
        "      cp.append(f)\n",
        "      j1 +=1\n",
        "     j +=1\n",
        "   j0 +=1\n",
        "   compedges.append(cp)\n",
        " return compedges\n",
        "\n",
        "# write edges into file\n",
        "\n",
        "def write_edge_reservoir(i,time_export):\n",
        "  with open('sample_data/%s_step_reservoir_edges.csv' % (time_export), 'w') as f:\n",
        "    f.write(\"window_counter,vertices,edges,Diameter,bitcoin_value\\n\")\n",
        "    writer = csv.writer(f, delimiter=',')\n",
        "    writer.writerows(i)\n",
        "    if time.time()*1000 > start + timeout:\n",
        "     f.close()\n",
        "    else:\n",
        "     print(\"\")\n",
        "  return True\n",
        "\n",
        "def write_edge_reservoir1(i, time_export):\n",
        "    f = open(\"sample_data/%s_step_reservoir_edges.csv\" % time_export, \"a\")\n",
        "    f.write(\"Diameter,vertices,edges,time,bitcoin_value\\n\")\n",
        "    # f.write(\"Source, Destination \\n\")\n",
        "    for j in i:\n",
        "      writer = csv.writer(f, delimiter=',')\n",
        "      writer.writerows(i)\n",
        "      f.close()\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peony-twitter[all]\n",
            "  Downloading peony_twitter-2.0.2-py3-none-any.whl (37 kB)\n",
            "Collecting aiohttp<4.0,>=2.0\n",
            "  Downloading aiohttp-3.8.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.1 MB/s \n",
            "\u001b[?25hCollecting async-timeout\n",
            "  Downloading async_timeout-4.0.0-py3-none-any.whl (6.1 kB)\n",
            "Collecting aiodns\n",
            "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-0.7.0-py3-none-any.whl (13 kB)\n",
            "Collecting python-magic\n",
            "  Downloading python_magic-0.4.24-py2.py3-none-any.whl (12 kB)\n",
            "Collecting cchardet\n",
            "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
            "\u001b[K     |████████████████████████████████| 263 kB 32.7 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0,>=2.0->peony-twitter[all]) (2.0.7)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0,>=2.0->peony-twitter[all]) (3.7.4.3)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0,>=2.0->peony-twitter[all]) (21.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 40.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=2.0->peony-twitter[all]) (2.10)\n",
            "Collecting pycares>=4.0.0\n",
            "  Downloading pycares-4.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 32.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pycares>=4.0.0->aiodns->peony-twitter[all]) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->peony-twitter[all]) (2.20)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pycares, aiohttp, python-magic, peony-twitter, cchardet, aiofiles, aiodns\n",
            "Successfully installed aiodns-3.0.0 aiofiles-0.7.0 aiohttp-3.8.0 aiosignal-1.2.0 async-timeout-4.0.0 asynctest-0.13.0 cchardet-2.1.7 frozenlist-1.2.0 multidict-5.2.0 peony-twitter-2.0.2 pycares-4.1.2 python-magic-0.4.24 yarl-1.7.2\n",
            "Requirement already satisfied: peony-twitter[all] in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: async-timeout in /usr/local/lib/python3.7/dist-packages (from peony-twitter[all]) (4.0.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from peony-twitter[all]) (3.8.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.7/dist-packages (from peony-twitter[all]) (0.4.24)\n",
            "Requirement already satisfied: cchardet in /usr/local/lib/python3.7/dist-packages (from peony-twitter[all]) (2.1.7)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.7/dist-packages (from peony-twitter[all]) (0.7.0)\n",
            "Requirement already satisfied: aiodns in /usr/local/lib/python3.7/dist-packages (from peony-twitter[all]) (3.0.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0,>=2.0->peony-twitter[all]) (2.0.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0,>=2.0->peony-twitter[all]) (21.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0,>=2.0->peony-twitter[all]) (5.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0,>=2.0->peony-twitter[all]) (3.7.4.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0,>=2.0->peony-twitter[all]) (1.7.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0,>=2.0->peony-twitter[all]) (1.2.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0,>=2.0->peony-twitter[all]) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp<4.0,>=2.0->peony-twitter[all]) (0.13.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=2.0->peony-twitter[all]) (2.10)\n",
            "Requirement already satisfied: pycares>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from aiodns->peony-twitter[all]) (4.1.2)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pycares>=4.0.0->aiodns->peony-twitter[all]) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->peony-twitter[all]) (2.20)\n",
            "Name: tornado\n",
            "Version: 5.1.1\n",
            "Summary: Tornado is a Python web framework and asynchronous networking library, originally developed at FriendFeed.\n",
            "Home-page: http://www.tornadoweb.org/\n",
            "Author: Facebook\n",
            "Author-email: python-tornado@googlegroups.com\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: \n",
            "Required-by: terminado, notebook, jupyter-client, ipykernel, google-colab, distributed, bokeh\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Collecting yahoo_fin\n",
            "  Downloading yahoo_fin-0.8.9.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from yahoo_fin) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from yahoo_fin) (2.23.0)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Collecting sgmllib3k\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->yahoo_fin) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->yahoo_fin) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->yahoo_fin) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->yahoo_fin) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->yahoo_fin) (3.0.4)\n",
            "Collecting w3lib\n",
            "  Downloading w3lib-1.22.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting parse\n",
            "  Downloading parse-1.19.0.tar.gz (30 kB)\n",
            "Collecting pyppeteer>=0.0.14\n",
            "  Downloading pyppeteer-0.2.6-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting pyquery\n",
            "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake-useragent-0.1.11.tar.gz (13 kB)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from requests-html->yahoo_fin) (0.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (4.62.3)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (1.4.4)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests-html->yahoo_fin) (4.8.1)\n",
            "Collecting websockets<10.0,>=9.1\n",
            "  Downloading websockets-9.1-cp37-cp37m-manylinux2010_x86_64.whl (103 kB)\n",
            "\u001b[K     |████████████████████████████████| 103 kB 41.6 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 55.1 MB/s \n",
            "\u001b[?25hCollecting pyee<9.0.0,>=8.1.0\n",
            "  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html->yahoo_fin) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html->yahoo_fin) (3.7.4.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->requests-html->yahoo_fin) (4.6.3)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.7/dist-packages (from pyquery->requests-html->yahoo_fin) (4.2.6)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: fake-useragent, parse, sgmllib3k\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-py3-none-any.whl size=13502 sha256=207d983fd80ce60807f9b3773d455c8fdcdfea8089d1cfa806da93e214a0e98e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/f7/62/50ab6c9a0b5567267ab76a9daa9d06315704209b2c5d032031\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.19.0-py3-none-any.whl size=24591 sha256=4c589d29b67b4ae1c915e606d1225091ec90635aa61e34fb8630a663c5a9cac9\n",
            "  Stored in directory: /root/.cache/pip/wheels/9c/aa/cc/f2228050ccb40f22144b073f15a2c84f11204f29fc0dce028e\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=a0c910bb17db3fd32594c3d724e87c0a1f9587941d0c998be9fbaf9415c4e80c\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
            "Successfully built fake-useragent parse sgmllib3k\n",
            "Installing collected packages: websockets, urllib3, pyee, cssselect, w3lib, sgmllib3k, pyquery, pyppeteer, parse, fake-useragent, requests-html, feedparser, yahoo-fin\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed cssselect-1.1.0 fake-useragent-0.1.11 feedparser-6.0.8 parse-1.19.0 pyee-8.2.2 pyppeteer-0.2.6 pyquery-1.4.3 requests-html-0.10.0 sgmllib3k-1.0.0 urllib3-1.25.11 w3lib-1.22.0 websockets-9.1 yahoo-fin-0.8.9.1\n",
            "Requirement already satisfied: requests_html in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: w3lib in /usr/local/lib/python3.7/dist-packages (from requests_html) (1.22.0)\n",
            "Requirement already satisfied: pyppeteer>=0.0.14 in /usr/local/lib/python3.7/dist-packages (from requests_html) (0.2.6)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (from requests_html) (0.0.1)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.7/dist-packages (from requests_html) (0.1.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from requests_html) (2.23.0)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.7/dist-packages (from requests_html) (1.19.0)\n",
            "Requirement already satisfied: pyquery in /usr/local/lib/python3.7/dist-packages (from requests_html) (1.4.3)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.8.1)\n",
            "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests_html) (8.2.2)\n",
            "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.25.11)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.62.3)\n",
            "Requirement already satisfied: websockets<10.0,>=9.1 in /usr/local/lib/python3.7/dist-packages (from pyppeteer>=0.0.14->requests_html) (9.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.7.4.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4->requests_html) (4.6.3)\n",
            "Requirement already satisfied: cssselect>0.7.9 in /usr/local/lib/python3.7/dist-packages (from pyquery->requests_html) (1.1.0)\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.7/dist-packages (from pyquery->requests_html) (4.2.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->requests_html) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->requests_html) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->requests_html) (2.10)\n",
            "Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from w3lib->requests_html) (1.15.0)\n",
            "Done with setup!\n",
            "If you'd like, you can click the (X) button to the left to clear this output.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBFNVbQ3NBEj"
      },
      "source": [
        "## The Twitter API "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FFngrFrM7sN"
      },
      "source": [
        "- Twitter is also known for being an abundant source of publc text data (perhaps even more so than Reddit).\n",
        "- Twitter provides several types of API that can be used to collect anything from tweets to user descriptions to follower networks.\n",
        "    - You can [read all about it here](https://developer.twitter.com/).\n",
        "- For this tutorial, we'll look at using the [streaming API](https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data.html), which allows us to retreive tweets that contain specific words, phrases, and hashtags.\n",
        "- In the slides, we talked about how to setup a Twitter App and get a API keys.\n",
        "    - You should add your own keys below and then run the code block to set your keys:\n",
        "    \n",
        "#### [Tutorial link to create an Twitter app](https://botwiki.org/resource/tutorial/how-to-create-a-twitter-app/) or https://www.youtube.com/watch?v=aB1x7ZsbbaQ&feature=youtu.be"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVXtia_JJJNu"
      },
      "source": [
        "\n",
        "# Go to http://dev.twitter.com and create an app. \n",
        "# The consumer key and secret will be generated for you after\n",
        "consumer_key=\"*********************\"\n",
        "consumer_secret=\"***************************************\"\n",
        "\n",
        "# After the step above, you will be redirected to your app's page.\n",
        "# Create an access token under the the \"Your access token\" section\n",
        "access_token=\"*****************************************\"\n",
        "access_token_secret=\"*************************************************\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ui2fEzDNbKc"
      },
      "source": [
        "- Do not share your credentials with anyone!\n",
        "    - You shouldn't hardcode your API keys in code (like above) if you are going to save the file anywhere that is visible to others (like commiting the file to github).\n",
        "        - You can read more about securing your API keys [here](https://developer.twitter.com/en/docs/basics/authentication/guides/securing-keys-and-tokens).\n",
        "     - So, if you plan to save this file in any way, make sure to remove your API keys first.\n",
        "     - If you think your keys have been compromized, you can regenerate them.\n",
        "        - [Apps](https://developer.twitter.com/en/apps) -> Keys and Tokens -> Regenerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuWUEnMkVm1Z"
      },
      "source": [
        "## Capture Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYNTJn478ue3"
      },
      "source": [
        "![Step reservoir sampling](https://github.com/GuillaumeVIMONT/TD_UP2_new/blob/master/static/img/continuous_sampling.png?raw=true)\n",
        "\n",
        "Before start Twitter capture we need some informations :\n",
        "\n",
        "* $k$ is the size of the reservoir \\\\\n",
        "* $tau$ is the lenght of the window \\\\\n",
        "* $lambda$ is the lenght of a strate \\\\\n",
        "* $tracking$ is the keywords to filter Twitter stream \\\\\n",
        "* $threshold$ is the threshold for the connected component export \\\\\n",
        "* $timeout$ is the duration of the capture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r0kIEhIQLAP"
      },
      "source": [
        "![DashboradUP2](https://github.com/alassou/TD-Up2/blob/master/Capture_dash.PNG?raw=true)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaDujAWy2t1E"
      },
      "source": [
        "\n",
        "#tickers (double-click to view) {display-mode: \"form\"}\n",
        "ticker = \"BTC-EUR\" #@param {type:\"string\"}\n",
        "ticker1 = \"ETH-USD\" #@param {type:\"string\"}\n",
        "nbp =  10 #@param {type:\"number\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jy0CtGEIdCI"
      },
      "source": [
        "#@title Capture Twitter streaming (double-click to view) {display-mode: \"form\"}\n",
        "\n",
        "k = 500 #@param {type:\"number\"}\n",
        "tau =  3#@param {type:\"number\"}\n",
        "lamb =  1#@param {type:\"number\"}\n",
        "tracking = \"Bitcoin\" #@param {type:\"string\"}\n",
        "threshold = 6 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "timeout = 360 #@param {type:\"slider\", min:1, max:360, step:1}\n",
        "timeout = int(timeout)*60000\n",
        "start = int(round(time.time() * 1000))\n",
        "\n",
        "global L, sample_window_stream, global_counter, time_counter, interval_counter\n",
        "L=0\n",
        "sample_window_stream = []\n",
        "global_counter = 0\n",
        "interval_counter = 0\n",
        "time_counter = (time.time()*1000)+60000\n",
        "\n",
        "\n",
        "# k is the size of the reservoir (number of edges into the reservoir)\n",
        "\n",
        "\n",
        "# tau is the lenght of the window\n",
        "tau =  tau*60000\n",
        "\n",
        "# lamb is the lenght of strate floato the window\n",
        "lamb = lamb*60000\n",
        "\n",
        "# rate is the number of strates per window\n",
        "rate = tau/lamb\n",
        "\n",
        "# M contain steps floato the window, for each strates we have a edges counter\n",
        "M = [0] * int(rate)\n",
        "\n",
        "# init it's a default parameters to configure somes variables when the reservoir strart\n",
        "init = 0\n",
        "\n",
        "reservoir = []\n",
        "# Using graph_objects\n",
        "import plotly.graph_objects as go\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "def diameter(i):\n",
        " comp = components(reservoir)\n",
        " comp1 = sorted(comp, reverse=True)\n",
        " #print(\"number of vertices in largest component\",comp1[0][0])\n",
        " #print(\"number of edges in largest component\",comp1[0][1])\n",
        " first=comp1[i]\n",
        " l=first[2]\n",
        " a=l[0]\n",
        "#print(\"First point\",a)\n",
        " dc={}\n",
        " dc[a]=0\n",
        " for b in d[a]: \n",
        "  if b in dc.keys(): pass\n",
        "  else: dc[b]=1\n",
        "  \n",
        "#Initialize S ans S1, Start iterating\n",
        " S=d[a]\n",
        " comp=[]\n",
        " S1=set([a])\n",
        " S=S.union(S1)\n",
        "#print(\"S=\",S)\n",
        " while S > S1:\n",
        "    S1=S\n",
        "    for u in S:\n",
        "       S=S.union(d[u])\n",
        "       for v in d[u]:\n",
        "         if v in dc.keys(): dc[v]=min(dc[v],dc[u]+1)\n",
        "         else: dc[v]=dc[u]+1\n",
        " dist=list(dc.values())\n",
        " ma=max(dist)\n",
        " for a, i in dc.items():\n",
        "   if i==ma: last=a\n",
        "   \n",
        "# we keep last  !!  \n",
        "# do it again !!\n",
        "\n",
        " a=last\n",
        " dc={}\n",
        " dc[a]=0\n",
        " for b in d[a]: \n",
        "  if b in dc.keys(): pass\n",
        "  else: dc[b]=1\n",
        "  \n",
        "#Initialize S ans S1, Start iterating\n",
        " S=d[a]\n",
        " comp=[]\n",
        " S1=set([a])\n",
        " S=S.union(S1)\n",
        "#print(\"S=\",S)\n",
        " while S > S1:\n",
        "    S1=S\n",
        "    for u in S:\n",
        "       S=S.union(d[u])\n",
        "       for v in d[u]:\n",
        "         if v in dc.keys(): dc[v]=min(dc[v],dc[u]+1)\n",
        "         else: dc[v]=dc[u]+1\n",
        " dist=list(dc.values())\n",
        " ma=max(dist)\n",
        " for a, i in dc.items():\n",
        "   if i==ma: last=a\n",
        "\n",
        " return(max(dist))\n",
        "TICKER_API_URL = 'https://api.coinmarketcap.com/v1/ticker/'\n",
        "def get_latest_crypto_price(crypto):\n",
        "  \n",
        "  response = requests.get(TICKER_API_URL+crypto)\n",
        "  response_json = response.json()\n",
        "  \n",
        "  return float(response_json[0]['price_usd'])\n",
        "def step_reservoir_sampling(edge):\n",
        "    global init, w_i, w_1, M, window_counter, lamb, t_i, dict_test, tau, t,hh\n",
        "    if init == 0:\n",
        "        # initialise w_i\n",
        "        w_1 = int(edge[2])+tau\n",
        "        w_i = int(edge[2])+tau\n",
        "        t_i = int(edge[2])+lamb\n",
        "        hh = []\n",
        "        # window counter\n",
        "        window_counter = 1\n",
        "        # terminate initialisation\n",
        "        init+=1\n",
        "    # check if edge is indide w_i\n",
        "    if int(edge[2]) < t_i:\n",
        "        M[-1]+=1\n",
        "    if int(edge[2]) >= t_i:\n",
        "        print(M)\n",
        "        #lst =[]\n",
        "        if int(edge[2]) >= w_1:\n",
        "          comp = components(reservoir)\n",
        "          comp1 = sorted(comp, reverse=True)\n",
        "          xx = comp_edges(comp1)\n",
        "          r = requests.get('https://api.coindesk.com/v1/bpi/currentprice.json')\n",
        "          bitcoin_data = dict(r.json())\n",
        "          bitcoin_value = bitcoin_data[\"bpi\"][\"USD\"][\"rate_float\"]\n",
        "          #Import the requests library\n",
        "          \n",
        "          x = comp1[0][0]\n",
        "          y = comp1[0][1]\n",
        "          z = diameter(0)\n",
        "          t= window_counter\n",
        "          #ticker = \"^FCHI\"\n",
        "          url = 'https://fr.finance.yahoo.com/quote/' + ticker\n",
        "          url1 = 'https://fr.finance.yahoo.com/quote/' + ticker1\n",
        "          \n",
        "          session = requests_html.HTMLSession()\n",
        "          session1 = requests_html.HTMLSession()\n",
        "          r = session.get(url)\n",
        "          r1 = session1.get(url1)\n",
        "          content1 = BeautifulSoup(r1.content, 'lxml')\n",
        "          content = BeautifulSoup(r.content, 'lxml')\n",
        "          uu1 = content1.select_one('.Mb\\(-4px\\)').text\n",
        "          #print(uu1)\n",
        "          #uu1 = float(uu1.replace(',',''))\n",
        "          uu = content.select_one('.Mb\\(-4px\\)').text\n",
        "          #uu = float(uu.replace(',',''))\n",
        "          ll =[(t,x,y,z,uu)]\n",
        "          ll1 =[(t,x,y,z,uu,uu1)]\n",
        "          if window_counter >= 1:\n",
        "            hh.append(ll1)\n",
        "          else:\n",
        "            print(\"\")\n",
        "\n",
        "          with open('sample_data/%s_step_reservoir_edges.csv', 'a') as f:\n",
        "            #f.write(\"window_counter,vertices,edges,Diameter,bitcoin_value\\n\")\n",
        "            writer = csv.writer(f, delimiter=',')\n",
        "            writer.writerows(ll1)\n",
        "            f.close()      \n",
        "          #print(\"Actual Bitcoin pricing \", bitcoin_value, \"$\") \n",
        "          #f window_counter == 3:\n",
        "           #x = comp1[0][0]\n",
        "           #y = comp1[0][1]\n",
        "           #z = diameter(0)\n",
        "           #t= window_counter\n",
        "           #print(\"Diameter =\",z,\",\",\"vertices =\",x,\",\",\"edges=\", y,\",\",\"time  =\",t, end='\\n')\n",
        "          #else:\n",
        "           #x = comp1[0][0]\n",
        "           #y = comp1[0][1]\n",
        "           #z = diameter(0)\n",
        "           #t= window_counter\n",
        "           #print(z,x,y,t,sep=',', end='\\n')\n",
        "          #print(\"Diameter of largest component  =\",diameter(0))\n",
        "          \n",
        "          #f.write(\"Diameter,vertices,edges,time\\n\")\n",
        "          #f.close()\n",
        "          #multiples_5 = [n for n in range(window_counter) if n % 5 == 0]\n",
        "\n",
        "          if (window_counter % nbp) == 0:\n",
        "            # Create figure with secondary y-axis\n",
        "            fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "            df = pd.read_csv('sample_data/%s_step_reservoir_edges.csv')\n",
        "            df = pd.read_csv('sample_data/%s_step_reservoir_edges.csv', names=['colA', 'colB','colC','colD','colE','colF'])\n",
        "            dm=df['colD']*5\n",
        "            fig.add_trace(go.Scatter(\n",
        "              x=df['colA'],\n",
        "              y=df['colE'],\n",
        "              name=\"ticker1 value\"\n",
        "            ))\n",
        "\n",
        "\n",
        "            fig.add_trace(go.Scatter(\n",
        "              x=df['colA'],\n",
        "              y=df['colF'],\n",
        "              name=\"ticker 2 value\",\n",
        "              yaxis=\"y2\"\n",
        "            ))\n",
        "\n",
        "            fig.add_trace(go.Scatter(\n",
        "              x=df['colA'],\n",
        "              y=df['colC'],\n",
        "              name=\"edge\",\n",
        "              yaxis=\"y3\"\n",
        "            ))\n",
        "\n",
        "            fig.add_trace(go.Scatter(\n",
        "              x=df['colA'],\n",
        "              y=df['colD'],\n",
        "              name=\"diameter\",\n",
        "              yaxis=\"y4\"\n",
        "            ))\n",
        "            fig.add_trace(go.Scatter(\n",
        "              x=df['colA'],\n",
        "              y=df['colB'],\n",
        "              name=\"vertices\",\n",
        "              yaxis=\"y3\"\n",
        "            ))\n",
        "\n",
        "\n",
        "            # Create axis objects\n",
        "            fig.update_layout(\n",
        "              template=\"plotly_dark\",\n",
        "              title={'text': 'UP2 Dashboard', 'font': {'color': 'white'}, 'x': 0.5},\n",
        "              margin={'b': 15},\n",
        "              hovermode='x',\n",
        "              autosize=True,\n",
        "\n",
        "              xaxis={'range': [df['colA'].index.min(), df['colA'].index.max()]},\n",
        "              yaxis=dict(\n",
        "                title=\"ticker1 axis\",\n",
        "                titlefont=dict(\n",
        "                  color=\"#1f77b4\"\n",
        "                  ),\n",
        "                tickfont=dict(\n",
        "                  color=\"#1f77b4\"\n",
        "                )\n",
        "              ),\n",
        "              yaxis2=dict(\n",
        "                title=\"ticker 2 axis\",\n",
        "                titlefont=dict(\n",
        "                  color=\"#d62728\"\n",
        "                ),\n",
        "                tickfont=dict(\n",
        "                  color=\"#d62728\"\n",
        "                ),\n",
        "                anchor=\"free\",\n",
        "                overlaying=\"y\",\n",
        "                side=\"left\",\n",
        "                position=0\n",
        "              ),\n",
        "              yaxis3=dict(\n",
        "                title=\"edge vertices axis\",\n",
        "                titlefont=dict(\n",
        "                  color=\"#32CD32\"\n",
        "                ),\n",
        "                tickfont=dict(\n",
        "                  color=\"#32CD32\"\n",
        "                ),\n",
        "                anchor=\"x\",\n",
        "                overlaying=\"y\",\n",
        "                side=\"right\"\n",
        "              ),\n",
        "              yaxis4=dict(\n",
        "                title=\"diameter  axis\",\n",
        "                titlefont=dict(\n",
        "                color=\"#9467bd\"\n",
        "                ),\n",
        "                tickfont=dict(\n",
        "                  color=\"#9467bd\"\n",
        "                ),\n",
        "                anchor=\"free\",\n",
        "                overlaying=\"y\",\n",
        "                side=\"right\",\n",
        "                position=1\n",
        "              )\n",
        "\n",
        "            )\n",
        "            fig.show()      \n",
        "\n",
        "          index = 0\n",
        "          for i in comp1:\n",
        "              if int(i[0]) >= int(threshold):\n",
        "                  #write_edge_reservoir(hh,time_export)\n",
        "                  index+=1\n",
        "              else:\n",
        "                  break\n",
        "        while len(reservoir) > 0 and int(reservoir[0][2]) < w_i-tau:\n",
        "            # while the previous condition is satisfy do\n",
        "            del reservoir[0]\n",
        "        window_counter +=1\n",
        "        del M[0]\n",
        "        M.append(1)\n",
        "        t_i += lamb\n",
        "        w_i += lamb\n",
        "        while len(reservoir) < 0 and int(reservoir[0][2]) > w_i-tau:\n",
        "          print(hh)\n",
        "    if sum(M) < k:\n",
        "        reservoir.append(edge)\n",
        "    else:\n",
        "        j = random.randint(0, sum(M))\n",
        "        if j < k:\n",
        "            if len(reservoir) < k:\n",
        "                reservoir.append(edge)\n",
        "            else:\n",
        "                del reservoir[j]\n",
        "                reservoir.append(edge)\n",
        "    \n",
        "    return reservoir\n",
        "\n",
        "\n",
        "# Get date\n",
        "now = datetime.datetime.now()\n",
        "def ask_exit():\n",
        "    for task in asyncio.Task.all_tasks():\n",
        "        task.cancel()\n",
        "async def consume(queue):\n",
        "    while True:\n",
        "        # wait for an item from the producer\n",
        "        item = await queue.get()\n",
        "        twitter_edges_graph = create_graph(item)\n",
        "        queue.task_done()\n",
        "        global time_counter, global_counter, interval_counter\n",
        "        if len(twitter_edges_graph) > 0:\n",
        "            # process the item\n",
        "            # print('consuming {}...'.format(twitter_edges_graph))\n",
        "            # Notify the queue that the item has been processed\n",
        "            for edge in twitter_edges_graph:\n",
        "                if tracking[0] in edge[1].lower():\n",
        "                    pass\n",
        "                else:\n",
        "                    global_counter+=1\n",
        "                    interval_counter+=1\n",
        "                    window_reservoir_sampling = step_reservoir_sampling(edge)\n",
        "        if time.time()*1000 > start + timeout:\n",
        "            print('Capture terminated')\n",
        "            ask_exit()\n",
        "\n",
        "\n",
        "\n",
        "class Client(PeonyClient):\n",
        "    pass\n",
        "\n",
        "# every class inheriting from `PeonyClient` or `BasePeonyClient` has\n",
        "# an event_stream function that can be used on an `EventStream`\n",
        "@Client.event_stream\n",
        "class UserStream(EventStream):\n",
        "\n",
        "    def stream_request(self):\n",
        "        \"\"\"\n",
        "            The stream_request method returns the request\n",
        "            that will be used by the stream\n",
        "        \"\"\"\n",
        "        return self.stream.statuses.filter.post(track=tracking)\n",
        "\n",
        "\n",
        "    # the on_connect event is triggered on connection to an user stream\n",
        "    # https://dev.twitter.com/streaming/overview/messages-types#friends-lists-friends\n",
        "    @events.on_connect.handler\n",
        "    def connection(self, data):\n",
        "        consumer = asyncio.ensure_future(consume(queue))\n",
        "        print(\"Connected to stream!\")\n",
        "\n",
        "    # the on_tweet event is triggered when a tweet seems to be sent on\n",
        "    # the stream, by default retweets are included\n",
        "    @events.on_tweet.handler\n",
        "    async def tweet(self, data):\n",
        "        await queue.put(data)\n",
        "        #print(\"producing\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    queue = asyncio.Queue()\n",
        "    client = Client(consumer_key=consumer_key,\n",
        "                     consumer_secret=consumer_secret,\n",
        "                     access_token=access_token,\n",
        "                     access_token_secret=access_token_secret)\n",
        "    client.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}